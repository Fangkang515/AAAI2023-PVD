## One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation
(Accepted by AAAI 2023)



## [项目视频](http://sk-fun.fun/PVD/) | [论文](https://arxiv.org/abs/2211.15977) | [数据集](https://drive.google.com/drive/folders/1U06KAEsW53PolLI3U8hWUhzzIH74QGaP?usp=sharing) | [预训练权重](https://drive.google.com/drive/folders/1GGJf-FTmpCJjmEn-AF_S9-HrLRkFe5Ud?usp=sharing) | [英文介绍](https://github.com/megvii-research/AAAI2023-PVD/blob/main/README.md) |

## 论文介绍

- 目前NeRF系列模型层出不穷，令人“眼花缭乱”，有基于隐式MLP的NeRF，基于纯显式张量结构的Plenoxels, 基于混合结构的TensoRF(低秩张量+MLP) 和 INGP(Hash+MLP)，以及其他各种变体。

- 不同结构的模型，其实特点是不一样。完全隐式的纯MLP结构，其高层语义特征可以拿来做很多事情，比如[光照/天气改变](https://nerf-w.github.io/)，[艺术设计](https://pfnet-research.github.io/distilled-feature-fields/)，而基于纯张量显式结构的模型，其空间结构清晰，容易进行[剪切/组合/放大/缩小/抹去](https://github.com/ashawkey/CCNeRF)等操作。TensoRF和INGP则介于两者之间，其优势更多在于训练快和重建质量质量高；此外硬件设施(如手机终端)对不同结构的支持度是完全不同的，选择什么样的结构用于下游任务需要一定的设计经验。

- 为了减轻设计者的选择痛苦，以及进行不同结构间的特性迁移，我们开展了结构转化的研究。此外不同结构间是否存在转化的可能性，目前尚未被研究，我们觉得首次尝试是有意义的。

- 我们的目标是将不同架构的特点能够转移到别的模型上。比如INGP快速收敛的特点能快速得到一个模型，进而可用蒸馏方式训练一个NeRF，也起到了加速效果，并且某些数据集上还能起到涨点的效果。此外还可以将显示结构的可编辑性转移到别的非显示结构上，比如对Plenoxels的张量结构进行场景组合，场景切分等操作，然后将其蒸馏到其他模型，使其它模型也具有渲染出编辑场景的效果。[我们的示例](http://sk-fun.fun/PVD/)

- 为何能进行蒸馏，能够为窥探这些模型的内部原理提供一些insight。比如中间feature可对齐，意味着不同结构间的模型可以映射到相近的空间。

## 安装
建议使用 [Anaconda](https://www.anaconda.com/) 进行安装，避免污染本机环境. 执行以下命令:

*Step1*: 创建名为 'pvd' 的conda 环境
```
conda create --name pvd python=3.7
conda activate pvd
pip install -r ./tools/requirements.txt
```
*Step2*: 安装C++/cuda扩展. (借鉴自 [torch-ngp](https://github.com/ashawkey/torch-ngp))
```
bash ./tools/install_extension.sh
```

## 数据集 & 预训练模型
Synthetic-NeRF/LLFF/Tanks&Temples： [google云盘](https://drive.google.com/drive/folders/1U06KAEsW53PolLI3U8hWUhzzIH74QGaP?usp=sharing), [baidu云盘](https://pan.baidu.com/s/1ky_TWrbUZG_MpHTBhncAKA?pwd=4h2h).

预训练模型： [google云盘](https://drive.google.com/drive/folders/1GGJf-FTmpCJjmEn-AF_S9-HrLRkFe5Ud?usp=sharing), [baidu云盘](https://pan.baidu.com/s/1LGLXwLGusX60GpAywLwosg?pwd=34k8).

不下载与训练模型，直接按照下面的方法训练一个teacher，也很快.

## 训练teacher
```
# train a hash-based(INGP) teacher
python main_just_train_tea.py ./data/nerf_synthetic/chair --model_type hash --data_type synthetic  --workspace ./log/train_teacher/hash_chair

# train a sparse-tensor-based(TensoRF VM-decomposion) teacher
python main_just_train_tea.py ./data/nerf_synthetic/chair --model_type vm --data_type synthetic  --workspace ./log/train_teacher/vm_chair

# train a MLP-based(NeRF) teacher
python main_just_train_tea.py ./data/nerf_synthetic/chair --model_type mlp --data_type synthetic  --workspace ./log/train_teacher/mlp_chair

# train a tensors-based(Plenoxels) teacher
python main_just_train_tea.py ./data/nerf_synthetic/chair --model_type tensors --data_type synthetic  --workspace ./log/train_teacher/tensors_chair

```

## 蒸馏模型
```
# teacher: hash(INGP),  student: vm(tensoRF)
python3 main_distill_mutual.py  ./data/nerf_synthetic/chair \
                    --data_type synthetic \
                    --teacher_type hash \
                    --ckpt_teacher ./log/train_teacher/hash_chair/checkpoints/XXX.pth \
                    --model_type vm \
                    --workspace ./log/distill_student/hash2vm/chair
                    
# teacher: MLP(NeRF),  student: tensors(Plenoxels)
python3 main_distill_mutual.py  ./data/nerf_synthetic/chair \
                    --data_type synthetic \
                    --teacher_type mlp \
                    --ckpt_teacher ./log/train_teacher/mlp_chair/checkpoints/XXX.pth \
                    --model_type tensors \
                    --workspace ./log/distill_student/mlp2tensors/chair
                   
```

## 测试

```
# evaluate a hash teacher
python main_distill_mutual.py ./data/nerf_synthetic/chair  --teacher_type hash --ckpt_teacher PATH/TO/CKPT.pth --test_teacher --data_type synthetic --workspace ./log/eval_teacher/hash_chair

# evaluate a mlp student
python main_distill_mutual.py ./data/nerf_synthetic/chair --model_type mlp --ckpt PATH/TO/CKPT.pth --test --data_type synthetic --workspace ./log/eval_student/mlp_chair
```

## 更多数据集上的使用以及更多运行命令参照如下
[more running description](https://github.com/megvii-research/AAAI2023-PVD/blob/main/tools/details.md)

## Citation

如果你觉得有用，可考虑引用我们的文章：
```
@article{pvd2023,
  author    = {Fang, Shuangkang and Xu, Weixin and Wang, Heng and Yang, Yi and Wang, Yufeng and Zhou, Shuchang},
  title     = {One is All: Bridging the Gap Between Neural Radiance Fields Architectures with Progressive Volume Distillation},
  journal   = {AAAI},
  year      = {2023}
}
```

### 致谢
感谢[ingp](https://github.com/NVlabs/instant-ngp),  [torch-ngp](https://github.com/ashawkey/torch-ngp), [TensoRF](https://github.com/apchenstu/TensoRF), [Plenoxels](https://github.com/sxyu/svox2), [nerf-pytorch](https://github.com/yenchenlin/nerf-pytorch)  的漂亮框架!
